{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import gensim.models as gsm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "data_train_path = './../data/train/train.csv'\n",
    "data_test_path = './../data/test/test.csv'\n",
    "\n",
    "# Word tools\n",
    "word_sentiments_intensity_path =  './../tools/word/emo_lex.csv'\n",
    "word_affect_intensity_path = './../tools/word/word_affect_intensity.csv'\n",
    "positive_words_path = './../tools/word/positive_words.txt'\n",
    "negative_words_path = './../tools/word/negative_words.txt'\n",
    "\n",
    "# Emoji tools\n",
    "emoji_polarity_path = './../tools/emoji/emoji_emotion.json'\n",
    "emoji_sentiments_path = './../tools/emoji/emoji_sentiments.csv'\n",
    "emoji_embedding_pre_trained_model_path = './../tools/emoji/emoji2vec.bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = TweetTokenizer()\n",
    "notstopwords = set(('not', 'can', 'no'))\n",
    "stopwords = set(nltk.corpus.stopwords.words('english')) - notstopwords\n",
    "\n",
    "standarizer_dict = {\n",
    "    r\"(http|https)?:\\/\\/[a-zA-Z0-9\\.-]+\\.[a-zA-Z]{2,4}(/\\S*)?\": \" <url> \",\n",
    "    r'(.)\\1+': r\"\\1\\1\", # cooool --> cool; coool--> cool\n",
    "    r\"\\'s\": \"\",\n",
    "    r\"\\'n\": \"\", \n",
    "    r\"\\'m\": \" am\", \n",
    "    r\"im\": \" \", \n",
    "    r\"\\'ve\": \" have\", \n",
    "    r\"\\'ve\": \" have\", \n",
    "    r\" can\\'t\": \" cannot\", \n",
    "    r\"n\\'t\": \" not\", \n",
    "    r\"\\'re\": \" are\", \n",
    "    r\"\\'d\": \" would\", \n",
    "    r\"\\'ll\": \" will\", \n",
    "    r\"\\.{1,1}\": \" \", \n",
    "    r\" [-+]?[.\\d]*[\\d]+[:,.\\d]*\": \"\",\n",
    "    r\"@\\w+\": r'  <entity> '\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    tweet.replace(\"\\\\n\", \" \")\n",
    "    # Standarize tweet\n",
    "    for current_form, standared_form in standarizer_dict.items():\n",
    "        tweet = re.sub(current_form, standared_form, tweet)\n",
    "    # Remove stop words\n",
    "    tweet = re.compile(r'\\b(' + r'|'.join(stopwords) + r')\\b\\s*').sub('', tweet)\n",
    "    # Lemmatization\n",
    "    tweet_tokens = []\n",
    "    for token, tag in pos_tag(tokenizer.tokenize(tweet)):\n",
    "        if tag[0].lower() in ['a','n','v']:\n",
    "            lem = lemmatizer.lemmatize(token,tag[0].lower())\n",
    "        else:\n",
    "            lem = lemmatizer.lemmatize(token)\n",
    "        \n",
    "        tweet_tokens.append(lem.lower())\n",
    "    return tweet_tokens\n",
    "\n",
    "def preprocess_features(df):\n",
    "    # Get labels (0,1,2,3) and polarity (sadness, joy, ..) \n",
    "    labels = sorted(set(df['class'].tolist()))\n",
    "    polarity = sorted(set(df['polarity'].tolist()))\n",
    "    one_hot = np.zeros((len(labels), len(labels)), int)\n",
    "    # Create onehot encoding for labels and popularities\n",
    "    np.fill_diagonal(one_hot, 1)\n",
    "    label_dict = dict(zip(labels, one_hot))\n",
    "    polarity_dict = dict(zip(polarity, one_hot))\n",
    "    return label_dict, polarity_dict\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    # Remove suffix of class : 0: no joy can be inferred -> 0\n",
    "    df['text'] = df['text'].apply(lambda x: preprocess_tweet(x)).tolist()\n",
    "    df['class'] = [c.split (\":\")[0] for c in df['class'].tolist()]\n",
    "    return df\n",
    "\n",
    "def prepare_cvs_data(file_path):\n",
    "    df = pd.read_csv(file_path, sep='\\t', header=None, encoding='utf-8', quoting=3)\n",
    "    df.columns = ['id','text','polarity','class'] # Set up column names\n",
    "    df = df.iloc[np.random.permutation(len(df))] # Random permutations\n",
    "    df = preprocess_dataframe(df)\n",
    "    return df\n",
    "\n",
    "def prepare_data(file_path):\n",
    "    df = prepare_cvs_data(file_path)\n",
    "    # Transform labels and polarities to onehot encoding\n",
    "    label_dict, polarity_dict = preprocess_features(df)\n",
    "    label = df['class'].apply(lambda y: label_dict[y]).tolist()\n",
    "    polarity = df['polarity'].apply(lambda y: polarity_dict[y]).tolist()\n",
    "    return df['text'], polarity, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets, test_labels, test_polarities = prepare_data(data_test_path)\n",
    "train_tweets, train_labels, train_polarities = prepare_data(data_train_path)\n",
    "test_data = list(zip(test_tweets, test_polarities, test_labels))\n",
    "train_data = list(zip(train_tweets, train_polarities, train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"😭😭 I \\n think that you've a lot looool money ;) @Singaholic121 Good morning, love! Happy first day of fall. Let's make some awesome #autumnmemories #annabailey\"\n",
    "x = preprocess_tweet(tweet)\n",
    "emoji = x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction\n",
    "## Words\n",
    "### Sentiment intensity\n",
    "- NRC Word-Emotion Association Lexicon (EmoLex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>true</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aaaaaaah</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aaaah</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abandon</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.703</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abandoned</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.534</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word  anger  anticipation  disgust   fear    joy  negative  positive  \\\n",
       "0       true  0.000           0.0      0.0  0.000  0.328       0.0       0.0   \n",
       "1   aaaaaaah  0.000           0.0      0.0  0.344  0.000       0.0       0.0   \n",
       "2      aaaah  0.000           0.0      0.0  0.234  0.000       0.0       0.0   \n",
       "3    abandon  0.000           0.0      0.0  0.531  0.000       0.0       0.0   \n",
       "4  abandoned  0.222           0.0      0.0  0.534  0.000       0.0       0.0   \n",
       "\n",
       "   sadness  surprise  trust  \n",
       "0    0.000       0.0    0.0  \n",
       "1    0.000       0.0    0.0  \n",
       "2    0.000       0.0    0.0  \n",
       "3    0.703       0.0    0.0  \n",
       "4    0.828       0.0    0.0  "
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_sentiment_intensity_df = pd.read_csv(word_sentiments_intensity_path, delimiter='\\t')\n",
    "word_sentiment_intensity_df['word'] = word_sentiment_intensity_df['word'].str.lower()\n",
    "word_sentiment_intensity_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For the following sentiments, the intensity value is always equals to 0.\n",
    "- We may remove these sentiment columns, because they will have no affect in the learning process.\n",
    "- Create a new dataframe with the useful columns (where sentiment intensity is different to 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Useful columns are: ['joy', 'sadness', 'word', 'fear', 'anger']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>word</th>\n",
       "      <th>fear</th>\n",
       "      <th>anger</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.328</td>\n",
       "      <td>0.000</td>\n",
       "      <td>true</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>aaaaaaah</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>aaaah</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.703</td>\n",
       "      <td>abandon</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.828</td>\n",
       "      <td>abandoned</td>\n",
       "      <td>0.534</td>\n",
       "      <td>0.222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     joy  sadness       word   fear  anger\n",
       "0  0.328    0.000       true  0.000  0.000\n",
       "1  0.000    0.000   aaaaaaah  0.344  0.000\n",
       "2  0.000    0.000      aaaah  0.234  0.000\n",
       "3  0.000    0.703    abandon  0.531  0.000\n",
       "4  0.000    0.828  abandoned  0.534  0.222"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "THRESHOLD = 100\n",
    "\n",
    "def get_useful_columns(df, class_column='word', threshold=0):\n",
    "    empty_columns = []\n",
    "    for column_name in list(df.columns):\n",
    "        if len(df.loc[df[column_name] != 0.]) <= threshold:\n",
    "            empty_columns.append(column_name)\n",
    "    useful_columns = list(set(df.columns) - set(empty_columns))\n",
    "    if class_column not in useful_columns:\n",
    "        useful_columns.append(class_column)\n",
    "    print(\"Useful columns are:\", useful_columns)\n",
    "    return useful_columns\n",
    "\n",
    "useful_columns = get_useful_columns(word_sentiment_intensity_df, threshold=THRESHOLD)\n",
    "word_sentiment_intensity_df = word_sentiment_intensity_df[useful_columns].copy()\n",
    "word_sentiment_intensity_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a dict of dict: {word: {sentiment: intensity, sent2: intensity, ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anger': 0.0, 'fear': 0.0, 'joy': 0.32799999999999996, 'sadness': 0.0}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_sentiment_intensity_dict = word_sentiment_intensity_df.set_index('word').T.to_dict()\n",
    "word_sentiment_intensity_dict['true']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final function\n",
    "## TODO:\n",
    "- check why useful columns are not working\n",
    "- Convert the float precision to only 3 values -> 0.333322 -> 0.333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Useful columns are: ['joy', 'sadness', 'word', 'fear', 'anger']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'anger': 0.0, 'fear': 0.0, 'joy': 0.32799999999999996, 'sadness': 0.0}"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_intensity_dict(dataset_path=word_sentiments_intensity_path,\n",
    "                                      delimiter='\\t', class_column='word',\n",
    "                                      remove_useless_columns=True, threshold=0):\n",
    "    df = pd.read_csv(dataset_path, delimiter=delimiter)\n",
    "    df[class_column] = df[class_column].str.lower()\n",
    "    if remove_useless_columns:\n",
    "        useful_columns = get_useful_columns(df, class_column=class_column, threshold=threshold)\n",
    "        df = df[useful_columns].copy()\n",
    "    df_dict = df.set_index(class_column).T.to_dict()\n",
    "    return df_dict\n",
    "\n",
    "word_sentiment_intensity_dict = get_intensity_dict()\n",
    "word_sentiment_intensity_dict['true']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positive Negatif words\n",
    "- Opinion Lexicon English (OLE)\n",
    "- https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bach/.local/lib/python3.6/site-packages/pandas/core/frame.py:882: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  \"columns will be omitted.\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'anger': 0.9640000000000001}"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_affect_dict = get_intensity_dict(\n",
    "    dataset_path=word_affect_intensity_path, class_column='term',\n",
    "    remove_useless_columns=False)\n",
    "\n",
    "new_dict = {}\n",
    "for word, affects in word_affect_dict.items():\n",
    "    new_dict[word] = {}\n",
    "    new_dict[word][affects['AffectDimension']] = affects['score']\n",
    "    \n",
    "word_affect_dict = new_dict\n",
    "word_affect_dict['outraged']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get positive and negative word list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006 4783\n"
     ]
    }
   ],
   "source": [
    "def get_file_content(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        content = f.readlines()\n",
    "    return content\n",
    "\n",
    "positive_word_list = get_file_content(positive_words_path)\n",
    "negative_word_list = get_file_content(negative_words_path)\n",
    "print(len(positive_word_list), len(negative_word_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emojis\n",
    "### 1 - Emoji polarity\n",
    "#### Emoji Valence (EV) \n",
    "- https://github.com/words/emoji-emotion/blob/master/index.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emoji_polarity_dict(emoji_data_path=emoji_polarity_path):\n",
    "    \"\"\"Returns a dict with the emoji unicode as key and the emoji polarity as value\"\"\"\n",
    "    with open(emoji_polarity_path) as json_data:\n",
    "        emoji_polarities = json.load(json_data)\n",
    "        \n",
    "    emoji_polarity_dict = dict()\n",
    "    for emoji_val in emoji_polarities:\n",
    "        emoji_polarity_dict[emoji_val[\"emoji\"]] = emoji_val[\"polarity\"]\n",
    "    return emoji_polarity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji_polarity_dict = get_emoji_polarity_dict()\n",
    "emoji_polarity_dict[emoji]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Emoji sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#Emoji</th>\n",
       "      <th>Unicode codepoint</th>\n",
       "      <th>Occurrences</th>\n",
       "      <th>Position</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Unicode name</th>\n",
       "      <th>Unicode block</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>😂</td>\n",
       "      <td>0x1f602</td>\n",
       "      <td>14622</td>\n",
       "      <td>0.805101</td>\n",
       "      <td>3614</td>\n",
       "      <td>4163</td>\n",
       "      <td>6845</td>\n",
       "      <td>FACE WITH TEARS OF JOY</td>\n",
       "      <td>Emoticons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>❤</td>\n",
       "      <td>0x2764</td>\n",
       "      <td>8050</td>\n",
       "      <td>0.746943</td>\n",
       "      <td>355</td>\n",
       "      <td>1334</td>\n",
       "      <td>6361</td>\n",
       "      <td>HEAVY BLACK HEART</td>\n",
       "      <td>Dingbats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>♥</td>\n",
       "      <td>0x2665</td>\n",
       "      <td>7144</td>\n",
       "      <td>0.753806</td>\n",
       "      <td>252</td>\n",
       "      <td>1942</td>\n",
       "      <td>4950</td>\n",
       "      <td>BLACK HEART SUIT</td>\n",
       "      <td>Miscellaneous Symbols</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>😍</td>\n",
       "      <td>0x1f60d</td>\n",
       "      <td>6359</td>\n",
       "      <td>0.765292</td>\n",
       "      <td>329</td>\n",
       "      <td>1390</td>\n",
       "      <td>4640</td>\n",
       "      <td>SMILING FACE WITH HEART-SHAPED EYES</td>\n",
       "      <td>Emoticons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>😭</td>\n",
       "      <td>0x1f62d</td>\n",
       "      <td>5526</td>\n",
       "      <td>0.803352</td>\n",
       "      <td>2412</td>\n",
       "      <td>1218</td>\n",
       "      <td>1896</td>\n",
       "      <td>LOUDLY CRYING FACE</td>\n",
       "      <td>Emoticons</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  #Emoji Unicode codepoint  Occurrences  Position  Negative  Neutral  \\\n",
       "0      😂           0x1f602        14622  0.805101      3614     4163   \n",
       "1      ❤            0x2764         8050  0.746943       355     1334   \n",
       "2      ♥            0x2665         7144  0.753806       252     1942   \n",
       "3      😍           0x1f60d         6359  0.765292       329     1390   \n",
       "4      😭           0x1f62d         5526  0.803352      2412     1218   \n",
       "\n",
       "   Positive                         Unicode name          Unicode block  \n",
       "0      6845               FACE WITH TEARS OF JOY              Emoticons  \n",
       "1      6361                    HEAVY BLACK HEART               Dingbats  \n",
       "2      4950                     BLACK HEART SUIT  Miscellaneous Symbols  \n",
       "3      4640  SMILING FACE WITH HEART-SHAPED EYES              Emoticons  \n",
       "4      1896                   LOUDLY CRYING FACE              Emoticons  "
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(emoji_sentiments_path, delimiter=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#Emoji</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>😂</td>\n",
       "      <td>3614</td>\n",
       "      <td>4163</td>\n",
       "      <td>6845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>❤</td>\n",
       "      <td>355</td>\n",
       "      <td>1334</td>\n",
       "      <td>6361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>♥</td>\n",
       "      <td>252</td>\n",
       "      <td>1942</td>\n",
       "      <td>4950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>😍</td>\n",
       "      <td>329</td>\n",
       "      <td>1390</td>\n",
       "      <td>4640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>😭</td>\n",
       "      <td>2412</td>\n",
       "      <td>1218</td>\n",
       "      <td>1896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  #Emoji  Negative  Neutral  Positive\n",
       "0      😂      3614     4163      6845\n",
       "1      ❤       355     1334      6361\n",
       "2      ♥       252     1942      4950\n",
       "3      😍       329     1390      4640\n",
       "4      😭      2412     1218      1896"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_columns = ['#Emoji', 'Negative', 'Neutral', 'Positive']\n",
    "emoji_sentiments_df = df.filter(target_columns, axis=1)\n",
    "emoji_sentiments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3614, 4163, 6845)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji_sentiments_dict = dict = emoji_sentiments_df.set_index('#Emoji').T.apply(tuple).to_dict()\n",
    "emoji_sentiments_dict['😂']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Emoji embedding\n",
    "#### emoji2vec\n",
    "- https://github.com/uclmr/emoji2vec/tree/master/pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03221022,  0.03802984, -0.00126745,  0.07279918, -0.02769   ,\n",
       "       -0.01013857,  0.0864341 , -0.01573726,  0.10124657,  0.08510233,\n",
       "        0.01168873, -0.06952818, -0.0263179 ,  0.10325162, -0.00335573,\n",
       "        0.04028838,  0.0177028 ,  0.06445294,  0.04212   , -0.09827866,\n",
       "        0.03254349,  0.1064738 ,  0.0735938 , -0.07274383,  0.01790767,\n",
       "        0.03611384,  0.02536146, -0.00573439,  0.06542501, -0.04325472,\n",
       "       -0.04252771, -0.00203998,  0.03511803, -0.09883551,  0.00640196,\n",
       "       -0.03072599,  0.05863559,  0.0196573 ,  0.03455479,  0.06133179,\n",
       "        0.08997498, -0.1033521 ,  0.0926978 , -0.10560492,  0.01345985,\n",
       "        0.01028653, -0.07663322,  0.04080227,  0.00948974,  0.09845643,\n",
       "       -0.05172281,  0.05844482,  0.03093685,  0.04788001,  0.07770577,\n",
       "       -0.05077488, -0.10048547,  0.00741034, -0.00987429, -0.04561597,\n",
       "       -0.02553738,  0.05784023, -0.03538289, -0.10472344,  0.04968144,\n",
       "        0.03641411, -0.00178819,  0.0460491 , -0.07111374,  0.05462777,\n",
       "        0.00840262,  0.01942872, -0.01409603,  0.08399905, -0.09584759,\n",
       "       -0.06943762,  0.04002187, -0.00200654,  0.10465576,  0.07935634,\n",
       "        0.04536102, -0.01173771,  0.01819525, -0.07027161, -0.10630365,\n",
       "        0.00028031, -0.02049813,  0.06458959,  0.00227968, -0.00458793,\n",
       "        0.03567093,  0.08418808,  0.01288835,  0.01090614, -0.07955473,\n",
       "       -0.02799193,  0.08273785,  0.02712864, -0.01900067, -0.07994583,\n",
       "       -0.09216195, -0.03080909,  0.05341667,  0.00671399,  0.0054099 ,\n",
       "       -0.01339956,  0.08811524,  0.01427626, -0.06198694, -0.02660287,\n",
       "       -0.03536662,  0.08076707,  0.09846219, -0.08837035,  0.10431933,\n",
       "       -0.04777761,  0.02928827, -0.04308478,  0.03285284,  0.06218312,\n",
       "       -0.04699458,  0.05258719, -0.07773928, -0.07233147, -0.03459608,\n",
       "       -0.06592723,  0.01457869, -0.00102549,  0.09438702, -0.05369863,\n",
       "       -0.11427967,  0.05766591, -0.01876533,  0.06612122, -0.03957663,\n",
       "       -0.00961796, -0.05622957,  0.10612873, -0.02046853, -0.0343483 ,\n",
       "       -0.00516819, -0.02303831,  0.03050318, -0.06206921, -0.05255537,\n",
       "        0.0283642 , -0.04039598, -0.02084247, -0.05805178, -0.10843672,\n",
       "        0.08030538,  0.0948353 ,  0.08223417,  0.07634503, -0.05526081,\n",
       "       -0.01768736,  0.0111634 ,  0.03004781, -0.07411735,  0.00932438,\n",
       "        0.02740203,  0.00273307,  0.10501628,  0.06219842,  0.08319105,\n",
       "       -0.08643461,  0.09420299,  0.02186613,  0.04084124, -0.01219009,\n",
       "       -0.01851739, -0.03444548, -0.09262411, -0.06472853, -0.03762739,\n",
       "       -0.06328289,  0.06421196,  0.04171333, -0.01253928, -0.03872873,\n",
       "       -0.04812841, -0.04947256,  0.00902338,  0.05584322, -0.09052408,\n",
       "        0.10012421, -0.03720468,  0.03227316,  0.03668046,  0.0782488 ,\n",
       "        0.03924715, -0.0551114 ,  0.06594816,  0.01013171,  0.01318165,\n",
       "        0.09956399, -0.08761751, -0.01000328,  0.01572178,  0.07250264,\n",
       "       -0.08701885,  0.00038182, -0.114664  , -0.04704453, -0.00156166,\n",
       "       -0.06499381, -0.086794  ,  0.06699998, -0.07895591, -0.05013448,\n",
       "       -0.03608249,  0.05218947, -0.05101082,  0.00423112, -0.07388374,\n",
       "       -0.019697  , -0.06951522,  0.02742959, -0.09637737, -0.10068464,\n",
       "        0.05893083,  0.11123026,  0.00194832,  0.03991983,  0.04294619,\n",
       "       -0.09284203, -0.00632637, -0.04572964,  0.00471146,  0.01591812,\n",
       "        0.04038478,  0.02931633, -0.09363055,  0.04071099,  0.07000662,\n",
       "        0.02026328, -0.02653382, -0.04921798,  0.0992802 ,  0.10970341,\n",
       "        0.11061528,  0.01882625, -0.07788266, -0.0197561 , -0.071175  ,\n",
       "        0.018868  ,  0.03231354, -0.00548712, -0.04071715, -0.0053359 ,\n",
       "        0.0028457 ,  0.05745151,  0.06901697,  0.05238566, -0.04511902,\n",
       "       -0.09585445,  0.00814678,  0.01691263,  0.01192293, -0.04480346,\n",
       "       -0.05335692, -0.02009244, -0.06181927,  0.08288597, -0.0263455 ,\n",
       "        0.07511866, -0.02172877,  0.04730215, -0.01258972,  0.00698932,\n",
       "       -0.05699366,  0.06699789,  0.08934435, -0.02703539,  0.04738401,\n",
       "       -0.04641952, -0.08647146,  0.11020926,  0.00100653,  0.01609659,\n",
       "        0.02886144, -0.0237805 ,  0.01535751, -0.00063221,  0.09141301,\n",
       "       -0.07135426, -0.03748184,  0.06656725, -0.01690095,  0.03272817,\n",
       "        0.02003223, -0.03031031, -0.0406027 , -0.03917235, -0.07859864,\n",
       "        0.0209902 , -0.05176842, -0.09077428,  0.06030557,  0.00753008], dtype=float32)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji2vec = gsm.KeyedVectors.load_word2vec_format(emoji_embedding_pre_trained_model_path, binary=True)\n",
    "happy_vector = emoji2vec[emoji]    # Produces an embedding vector of length 300\n",
    "happy_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Create a dict for all emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
