{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_path = './../data/train/train.csv'\n",
    "data_test_path = './../data/test/test.csv'\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = TweetTokenizer()\n",
    "notstopwords = set(('not', 'can', 'no'))\n",
    "stopwords = set(nltk.corpus.stopwords.words('english')) - notstopwords\n",
    "\n",
    "standarizer_dict = {\n",
    "    r\"(http|https)?:\\/\\/[a-zA-Z0-9\\.-]+\\.[a-zA-Z]{2,4}(/\\S*)?\": \" <url> \",\n",
    "    r'(.)\\1+': r\"\\1\\1\", # cooool --> cool; coool--> cool\n",
    "    r\"\\'s\": \"\",\n",
    "    r\"\\'n\": \"\", \n",
    "    r\"\\'m\": \" am\", \n",
    "    r\"im\": \" \", \n",
    "    r\"\\'ve\": \" have\", \n",
    "    r\"\\'ve\": \" have\", \n",
    "    r\" can\\'t\": \" cannot\", \n",
    "    r\"n\\'t\": \" not\", \n",
    "    r\"\\'re\": \" are\", \n",
    "    r\"\\'d\": \" would\", \n",
    "    r\"\\'ll\": \" will\", \n",
    "    r\"\\.{1,1}\": \" \", \n",
    "    r\" [-+]?[.\\d]*[\\d]+[:,.\\d]*\": \"\",\n",
    "    r\"@\\w+\": r'  <entity> '\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    tweet.replace(\"\\\\n\", \" \")\n",
    "    # Standarize tweet\n",
    "    for current_form, standared_form in standarizer_dict.items():\n",
    "        tweet = re.sub(current_form, standared_form, tweet)\n",
    "    # Remove stop words\n",
    "    tweet = re.compile(r'\\b(' + r'|'.join(stopwords) + r')\\b\\s*').sub('', tweet)\n",
    "    # Lemmatization\n",
    "    tweet_tokens = []\n",
    "    for token, tag in pos_tag(tokenizer.tokenize(tweet)):\n",
    "        if tag[0].lower() in ['a','n','v']:\n",
    "            lem = lemmatizer.lemmatize(token,tag[0].lower())\n",
    "        else:\n",
    "            lem = lemmatizer.lemmatize(token)\n",
    "        \n",
    "        tweet_tokens.append(lem.lower())\n",
    "    return tweet_tokens\n",
    "\n",
    "def preprocess_features(df):\n",
    "    # Get labels (0,1,2,3) and polarity (sadness, joy, ..) \n",
    "    labels = sorted(set(df['class'].tolist()))\n",
    "    polarity = sorted(set(df['polarity'].tolist()))\n",
    "    one_hot = np.zeros((len(labels), len(labels)), int)\n",
    "    # Create onehot encoding for labels and popularities\n",
    "    np.fill_diagonal(one_hot, 1)\n",
    "    label_dict = dict(zip(labels, one_hot))\n",
    "    polarity_dict = dict(zip(polarity, one_hot))\n",
    "    return label_dict, polarity_dict\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    # Remove suffix of class : 0: no joy can be inferred -> 0\n",
    "    df['text'] = df['text'].apply(lambda x: preprocess_tweet(x)).tolist()\n",
    "    df['class'] = [c.split (\":\")[0] for c in df['class'].tolist()]\n",
    "    return df\n",
    "\n",
    "def prepare_cvs_data(file_path):\n",
    "    df = pd.read_csv(file_path, sep='\\t', header=None, encoding='utf-8', quoting=3)\n",
    "    df.columns = ['id','text','polarity','class'] # Set up column names\n",
    "    df = df.iloc[np.random.permutation(len(df))] # Random permutations\n",
    "    df = preprocess_dataframe(df)\n",
    "    return df\n",
    "\n",
    "def prepare_data(file_path):\n",
    "    df = prepare_cvs_data(file_path)\n",
    "    # Transform labels and polarities to onehot encoding\n",
    "    label_dict, polarity_dict = preprocess_features(df)\n",
    "    label = df['class'].apply(lambda y: label_dict[y]).tolist()\n",
    "    polarity = df['polarity'].apply(lambda y: polarity_dict[y]).tolist()\n",
    "    return df['text'], polarity, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets, test_labels, test_polarities = prepare_data(data_test_path)\n",
    "train_tweets, train_labels, train_polarities = prepare_data(data_train_path)\n",
    "test_data = list(zip(test_tweets, test_polarities, test_labels))\n",
    "train_data = list(zip(train_tweets, train_polarities, train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"ðŸ˜­ðŸ˜­ I \\n think that you've a lot looool money ;) @Singaholic121 Good morning, love! Happy first day of fall. Let's make some awesome #autumnmemories #annabailey\"\n",
    "preprocess_tweet(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
